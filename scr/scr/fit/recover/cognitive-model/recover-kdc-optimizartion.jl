path2root = dirname(Base.active_project());
joinpath(path2root, "scr", "scr", "utils.jl") |> include;
joinpath(path2root, "scr", "scr", "model", "ð·.jl") |> include;
joinpath(path2root, "scr", "scr", "model", "model.jl") |> include;
joinpath(path2root, "scr", "scr", "fit", "recover", "utils.jl") |> include;
using Functors, StatsFuns, JLD2, HybridModels, Optim, NNlib;
using CSV, DataFrames;
using Flux: logitcrossentropy, softmax;


println("\nLoading Data...")
path2data = joinpath(path2root, "data", "preprocessed", "binary");
ct1 = CSV.read( joinpath( path2data, "ct1.csv"), DataFrame );
trials = vcat([
    begin
        sbjdf = filter(r -> r.subject==subject, ct1);
        trials = vcat(
            [ # Trials
                sbjdf |> 
                filter(r -> r.trial==trial && r.event in ["switch", "stay", "select"] && r.visit>0) |> 
                df -> Trial(df) for trial in sbjdf.trial |> unique
            ]
        );
        trials
    end
    for subject in 1:15
]...);
d = vcat([create_dataframe(trial) for trial in trials]...);


println("Set Model and Parameters")
@hybridmodel function generative_model(X)
    # --- Transform and Extract Parameters --- #
    @kdc begin
        Î»â‚€ = logit(0.99f0) 
        Ï‰  = logit(0.60f0) 
        Îºâ‚ = logit(0.25f0) 
        Î»â‚‚ = logit(0.01f0) 
        Ï„  = logit(0.08f0) 
    end
    @ddc Î² = 1.2f0

    
    # --- Transform parameters --- #
    Î»â‚€, Ï‰, Îºâ‚, Î»â‚‚, Ï„ = Ïƒ(Î»â‚€), Ïƒ(Ï‰), Ïƒ(Îºâ‚), Ïƒ(Î»â‚‚), Ïƒ(Ï„);

    # --- Compute Î± and Î² to establish the shape of the beta posterior distribution --- #
    nL = n0L(X) .+ n1_0L(X, Ï‰)      .+ n1_1L(X) .+ n2_0L(X, Î»â‚‚) .+ n2_1L(X)
    rL = r0L(X) .+ r1_0L(X, Î»â‚€, nL) .+ r1_1L(X) .+ r2_0L(X, Î»â‚‚) .+ r2_1L(X)

    nR = n0R(X) .+ n1_0R(X, Ï‰)      .+ n1_1R(X) .+ n2_0R(X, Î»â‚‚) .+ n2_1R(X)
    rR = r0R(X) .+ r1_0R(X, Î»â‚€, nR) .+ r1_1R(X) .+ r2_0R(X, Î»â‚‚) .+ r2_1R(X)

    # --- Value of Select --- #    
    ÏL = rL ./ nL
    ÏR = rR ./ nR

    # --- Value of Information --- #    
    voiL = ucb([nL nR]', Î²)'
    voiR = ucb([nR nL]', Î²)'

    # --- Cost of Information --- #    
    coiL = (1f0 .- X.gL) .* Îºâ‚
    coiR = (1f0 .- X.gR) .* Îºâ‚

    return [(voiL .- coiL) (voiR .- coiR) (ÏL .- ÏR) (ÏR .- ÏL)]' ./ Ï„
end;


println("Simulate data")
pact = generative_model(d) |> softmax;
d.act = [rand( Distributions.Categorical(p[:]) ) for p in eachcol(pact)];
(train_data, test_data) = splitobs(d, at=0.7);
data = (;
    train = (; X=train_data, y=onehotbatch(train_data.act, 1:4)),
    test  = (; X=test_data, y=onehotbatch(test_data.act, 1:4))
);


println("Randomly Initialize Parameters")
Î¸init = [
    -(rand(Float32) + 1),  # Îºâ‚
    randn(Float32) |> abs, # Ï‰
    -(rand(Float32) - 1),  # Ï„
    -(rand(Float32) + 4),  # Î»â‚‚
    randn(Float32) |> abs, # Î»â‚€
    # randn(Float32) |> abs, # Î²
];


println("Optimize Parameters")
# Define loss function to take Î¸ and return a scalar
function loss(Î¸)
    predictions = (m)(Î¸, data.train.X)
    return logitcrossentropy(predictions, data.train.y)
end;
opt_prob = Optim.optimize(loss, Î¸init, LBFGS()); # Create an optimization problem
Î¸_opt = Optim.minimizer(opt_prob); # Get the optimized parameters
# Print the results
println("Optimization results:")
Î¸_opt_dict = Dict(m.kdc.names[i] => Ïƒ(Î¸_opt[i]) for i in 1:length(m.kdc.names));
println("Minimized loss: ", Optim.minimum(opt_prob))
println("Optimized parameters: "); display(Î¸_opt_dict);

# Create Dict using vector of Symbols and vector of values

